{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6901d0e8-3721-45c6-9088-eeebcc29391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torchtext\n",
    "#%pip install torch-model-archiver\n",
    "#%pip install --user pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0504cd0c-7596-4c59-9df7-5bd8b26fc200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b4f7d32-9ed1-4909-a013-d014ee04f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import SGD\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649dd165-f916-4b47-99f4-07fc22ca9322",
   "metadata": {},
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62070fe9-1917-485e-b279-3403360667fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "# PROJECT_ID = \"qwiklabs-asl-00-748dac0c969e\"\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-ner-kfp-artifact-store\"\n",
    "\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "TESTING_FILE_PATH = f\"{DATA_ROOT}/testing/dataset.csv\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c836fb7-3c20-4458-9df2-8cb73c4f0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_DIR_ROOT\"] = JOB_DIR_ROOT\n",
    "os.environ[\"TRAINING_FILE_PATH\"] = TRAINING_FILE_PATH\n",
    "os.environ[\"VALIDATION_FILE_PATH\"] = VALIDATION_FILE_PATH\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c06492d1-cc43-442f-9aa4-b7df02788ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-00-69fe165840f7-ner-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c05a6-19af-467e-8c3a-8bd99e6f39bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save NER granular data from source to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5797f669-17d0-4704-ad84-0b745721570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0613 22:55:06.874674 140063327733568 bigquery_client.py:731] There is no apilog flag so non-critical logging is disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_granular_dataset already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0613 22:55:08.048714 140614478464832 bigquery_client.py:731] There is no apilog flag so non-critical logging is disabled.\n",
      "Waiting on bqjob_r71c11e291affb208_00000188b6f85511_1 ... (2s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=ner_granular_dataset\n",
    "TABLE_ID=ner_granular\n",
    "DATA_SOURCE=gs://quicklabs-asl-ner-dataset/ner.csv\n",
    "SCHEMA=text:STRING,\\\n",
    "labels:STRING\n",
    "\n",
    "exists=$(bq ls -d | grep -w $DATASET_ID)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"$DATASET_ID already exists\"\n",
    "else\n",
    "   echo \"Creating $dataset\"\n",
    "   bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "fi\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a327e03-914d-4dd6-a0c1-c4d80e702236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4149cb03382d4c8cadcc26d17ec006f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1351aea4da427db2a2fa99ac787717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery df\n",
    "SELECT *\n",
    "FROM `ner_granular_dataset.ner_granular`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e58edcc-463f-426e-b93c-34077ebcbfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02bb66d78124a1ab6c65f0a9f977c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5064c69d97684a70a9c11e446a0bc6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---------------------- forwarded by phillip k ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greg, something that i forgot to ask you.  do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lucy, here is a draft of a memo we should dist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brad,  with regard to tori kuykendall, i would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>04/27/2001 01:01 pm a current notes user: rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>how am i to send them the money for the silent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>i will email you an updated operating statemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>ray, is there any detail on the gas cost proxy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>---------------------- forwarded by phillip k ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    ---------------------- forwarded by phillip k ...\n",
       "1                                                   no\n",
       "2    greg, something that i forgot to ask you.  do ...\n",
       "3    lucy, here is a draft of a memo we should dist...\n",
       "4    brad,  with regard to tori kuykendall, i would...\n",
       "..                                                 ...\n",
       "493   04/27/2001 01:01 pm a current notes user: rea...\n",
       "494  how am i to send them the money for the silent...\n",
       "495  i will email you an updated operating statemen...\n",
       "496  ray, is there any detail on the gas cost proxy...\n",
       "497  ---------------------- forwarded by phillip k ...\n",
       "\n",
       "[498 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `qwiklabs-asl-00-748dac0c969e.asl_team3.enron_mail_body_500`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2585dc-4e4c-40af-ba83-f3eec354a53d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## training and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e8ef1993-cf79-4fee-9959-36919a749cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Questions ?</td>\n",
       "      <td>O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was .</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifteen dollars .</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text labels\n",
       "0                ...      O\n",
       "1                The      O\n",
       "2        Questions ?    O O\n",
       "3           It was .  O O O\n",
       "4  Fifteen dollars .  O O O"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5f07c3e4-e456-4452-81f2-46af6c6e6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[np.random.permutation(len(df))][0:5000]\n",
    "\n",
    "labels = [i.split() for i in df['labels'].values.tolist()]\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "        [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(unique_labels)}\n",
    "\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e980b3bc-90c3-4409-97db-1a5bf4de3cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "45138658-b8a6-4fc0-a7be-eb76e061b387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28282</th>\n",
       "      <td>It would be her third stint in rehab , followi...</td>\n",
       "      <td>O O O O O O O O O O O O O B-geo O B-geo O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47574</th>\n",
       "      <td>The International Committee of the Red Cross s...</td>\n",
       "      <td>O B-org I-org O O B-geo I-geo O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>What is new is the idea of putting the solar p...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>Authorities say the suspects were planning a m...</td>\n",
       "      <td>O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23146</th>\n",
       "      <td>Analysts predict the spending surge will encou...</td>\n",
       "      <td>O O O O O O O O B-geo O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "28282  It would be her third stint in rehab , followi...   \n",
       "47574  The International Committee of the Red Cross s...   \n",
       "18599  What is new is the idea of putting the solar p...   \n",
       "1666   Authorities say the suspects were planning a m...   \n",
       "23146  Analysts predict the spending surge will encou...   \n",
       "\n",
       "                                                  labels  \n",
       "28282  O O O O O O O O O O O O O B-geo O B-geo O O B-...  \n",
       "47574  O B-org I-org O O B-geo I-geo O O O O O O O O ...  \n",
       "18599    O O O O O O O O O O O O O O O O O O O O O O O O  \n",
       "1666                                 O O O O O O O O O O  \n",
       "23146  O O O O O O O O B-geo O O O O O O O O O O O O ...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58535a-11f2-4030-8611-e94e27c11913",
   "metadata": {
    "tags": []
   },
   "source": [
    "## train/val/test data save to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ef5739b-0e66-4e9f-adb6-e9880bab6247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas_gbq\n",
    "\n",
    "try:\n",
    "    pandas_gbq.to_gbq(df_train, \"ner_granular_dataset.training\", project_id=PROJECT_ID)\n",
    "except:\n",
    "    print(\"table already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "318029ee-fd92-4f69-a13a-d8e57f803dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "W0613 23:20:16.496857 140601218156352 bigquery_client.py:731] There is no apilog flag so non-critical logging is disabled.\n",
      "Waiting on bqjob_r5c4812b15a241663_00000188b70f5971_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "ner_granular_dataset.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d639cab1-05b1-404d-aeb3-f96caf6cb47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9489.38it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pandas_gbq.to_gbq(df_val, \"ner_granular_dataset.validation\", project_id=PROJECT_ID)\n",
    "except:\n",
    "    print(\"table already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6d05c19-0d6d-4296-a5ef-d50b1d8f4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "W0613 23:20:27.614960 139728485074752 bigquery_client.py:731] There is no apilog flag so non-critical logging is disabled.\n",
      "Waiting on bqjob_r62cd9e6ae0e182c_00000188b70f84df_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "ner_granular_dataset.validation \\\n",
    "$VALIDATION_FILE_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d259ebb5-4fd0-471c-af64-f1adcb5ed67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8793.09it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pandas_gbq.to_gbq(df_test, \"ner_granular_dataset.testing\", project_id=PROJECT_ID)\n",
    "except:\n",
    "    print(\"table already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ce9f996-6a1e-4151-9324-c8d37813f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "W0613 23:20:33.166322 139964956174144 bigquery_client.py:731] There is no apilog flag so non-critical logging is disabled.\n",
      "Waiting on bqjob_r1b4353a61aa4e907_00000188b70f9a8f_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "ner_granular_dataset.testing \\\n",
    "$TESTING_FILE_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc4e51-28e5-4f63-80b0-9498f1d06c87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f56fdd4d-fb04-40a2-bf6e-207a46d79c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "df_test = pd.read_csv(TESTING_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "99e66c7e-4555-4ed9-a6a5-776d2ca10e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other support it .</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He was 82 .</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No casualties were reported .</td>\n",
       "      <td>O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One person was injured .</td>\n",
       "      <td>O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text     labels\n",
       "0                            The          O\n",
       "1             Other support it .    O O O O\n",
       "2                    He was 82 .    O O O O\n",
       "3  No casualties were reported .  O O O O O\n",
       "4       One person was injured .  O O O O O"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a9fb702-a865-44b8-8850-10847bf858ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cbb19f77-6948-4307-b957-92f854607005",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = False\n",
    "\n",
    "def align_label(texts, labels):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
    "        txt = df['text'].values.tolist()\n",
    "        self.texts = [tokenizer(str(i),\n",
    "                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n",
    "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        \n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a801bd-f082-47da-978e-91e33506b60e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## build bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50f54829-ebed-416a-9fa2-0b23fbc5ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bcc9feb1-c73b-4cc0-834c-f401a3608e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    #select predicted index with maximum logit for each token\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "75aac702-fe25-4665-a185-aea20e72b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eva_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c78454-f770-4717-894d-5f237787a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 59%|█████▉    | 474/800 [03:19<02:16,  2.38it/s]"
     ]
    }
   ],
   "source": [
    "def train_loop(model, df_train, df_val):\n",
    "\n",
    "    train_dataset = DataSequence(df_train)\n",
    "    val_dataset = DataSequence(df_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][train_label[i] != -100]\n",
    "              label_clean = train_label[i][train_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_train += acc\n",
    "              total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        \n",
    "        for val_data, val_label in val_dataloader:\n",
    "\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = val_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            loss, logits = model(input_id, mask, val_label)\n",
    "            \n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][val_label[i] != -100]\n",
    "              label_clean = val_label[i][val_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_val += acc\n",
    "              total_loss_val += loss.item()\n",
    "                \n",
    "        f1_metric.add_batch(predictions=predictions, references=label_clean)\n",
    "        val_accuracy = total_acc_val / len(df_val)\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "        f1 = f1_metric.compute(average='micro')\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "        print(f'f1: {f1}')\n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "model = BertModel()\n",
    "train_loop(model, df_train, df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e921dbb-7918-4c25-b324-1b535a6559fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "53d5b783-8860-4f1c-81da-39678c85c990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Accuracy:  0.932\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, df_test):\n",
    "\n",
    "    test_dataset = DataSequence(df_test)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0.0\n",
    "\n",
    "    for test_data, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "            input_id = test_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            loss, logits = model(input_id, mask, test_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][test_label[i] != -100]\n",
    "              label_clean = test_label[i][test_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_test += acc\n",
    "\n",
    "    val_accuracy = total_acc_test / len(df_test)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
    "\n",
    "\n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d2605-3ea1-4309-b405-6db23ae0b714",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d201c7b5-f77d-418b-8f79-2ed0a85892b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## prediction on single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "608cc2cc-839c-4f19-8a17-ac8533654586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_word_ids(texts):\n",
    "  \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    mask = text['attention_mask'].to(device)\n",
    "    input_id = text['input_ids'].to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "    print(sentence)\n",
    "    print(prediction_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "624ff203-cd91-4a2c-a227-3166f091cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Gates is the founder of Microsoft at Seattle, Washionton\n",
      "['B-per', 'I-per', 'O', 'O', 'O', 'O', 'B-org', 'O', 'B-geo', 'O', 'B-org']\n",
      "Dear Jim, thanks for booking at Marroit at San Jose, California on May 15th, 2023\n",
      "['O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'B-geo', 'I-geo', 'O', 'I-geo', 'O', 'I-tim', 'I-tim', 'I-tim', 'I-tim']\n",
      "Dear Mary, here is your shopping summary at Macy's at San Jose, California\n",
      "['O', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'I-geo', 'I-geo', 'O', 'O']\n",
      "Dear Larry, here is your united airline ticket departing from San Francisco Airpot on March 12, 2013\n",
      "['O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'I-org', 'O', 'B-tim', 'I-tim', 'I-tim', 'I-tim']\n",
      "Starting in late October, we'll launch new flights from Los Angeles to Auckland and begin daily service from San Francisco to Auckland.\n",
      "['O', 'O', 'B-tim', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'B-geo', 'O']\n"
     ]
    }
   ],
   "source": [
    "evaluate_one_text(model, 'Bill Gates is the founder of Microsoft at Seattle, Washionton')\n",
    "evaluate_one_text(model, 'Dear Jim, thanks for booking at Marroit at San Jose, California on May 15th, 2023')\n",
    "evaluate_one_text(model, \"Dear Mary, here is your shopping summary at Macy's at San Jose, California\")\n",
    "evaluate_one_text(model, \"Dear Larry, here is your united airline ticket departing from San Francisco Airpot on March 12, 2013\")\n",
    "evaluate_one_text(model,\"Starting in late October, we'll launch new flights from Los Angeles to Auckland and begin daily service from San Francisco to Auckland.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c944c8-f1f8-4ecd-b1f8-e4432b94475a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a95af34e-388b-472b-a58e-dac704edc80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = \"training_app\"\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00ade8c1-363b-4606-8d4e-7a1c63c8a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import SGD\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "label_all_tokens = False\n",
    "\n",
    "def align_label(texts, labels):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
    "        txt = df['text'].values.tolist()\n",
    "        self.texts = [tokenizer(str(i),\n",
    "                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n",
    "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        \n",
    "        return batch_data, batch_labels\n",
    "\n",
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=17)\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output \n",
    "    \n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def train_evaluate(job_dir, TRAINING_FILE_PATH, VALIDATION_FILE_PATH, TESTING_FILE_PATH, batch_size, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "    df_val = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "    df_test = pd.read_csv(TESTING_FILE_PATH)\n",
    "\n",
    "    def train_loop(model, df_train, df_val):\n",
    "\n",
    "        train_dataset = DataSequence(df_train)\n",
    "        val_dataset = DataSequence(df_val)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "        optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if use_cuda:\n",
    "            model = model.cuda()\n",
    "\n",
    "        best_acc = 0\n",
    "        best_loss = 1000\n",
    "\n",
    "        for epoch_num in range(EPOCHS):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "                input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "                for i in range(logits.shape[0]):\n",
    "\n",
    "                  logits_clean = logits[i][train_label[i] != -100]\n",
    "                  label_clean = train_label[i][train_label[i] != -100]\n",
    "\n",
    "                  predictions = logits_clean.argmax(dim=1)\n",
    "                  acc = (predictions == label_clean).float().mean()\n",
    "                  total_acc_train += acc\n",
    "                  total_loss_train += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            for val_data, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "                input_id = val_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "                for i in range(logits.shape[0]):\n",
    "\n",
    "                  logits_clean = logits[i][val_label[i] != -100]\n",
    "                  label_clean = val_label[i][val_label[i] != -100]\n",
    "\n",
    "                  predictions = logits_clean.argmax(dim=1)\n",
    "                  acc = (predictions == label_clean).float().mean()\n",
    "                  total_acc_val += acc\n",
    "                  total_loss_val += loss.item()\n",
    "\n",
    "            f1_metric.add_batch(predictions=predictions, references=label_clean)\n",
    "            val_accuracy = total_acc_val / len(df_val)\n",
    "            val_loss = total_loss_val / len(df_val)\n",
    "            f1 = f1_metric.compute(average='micro')\n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "            print(f'f1: {f1}')\n",
    "\n",
    "    def evaluate(model, df_test):\n",
    "\n",
    "        test_dataset = DataSequence(df_test)\n",
    "\n",
    "        test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "        if use_cuda:\n",
    "            model = model.cuda()\n",
    "\n",
    "        total_acc_test = 0.0\n",
    "\n",
    "        for test_data, test_label in test_dataloader:\n",
    "\n",
    "                test_label = test_label.to(device)\n",
    "                mask = test_data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "                input_id = test_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                loss, logits = model(input_id, mask, test_label)\n",
    "\n",
    "                for i in range(logits.shape[0]):\n",
    "\n",
    "                  logits_clean = logits[i][test_label[i] != -100]\n",
    "                  label_clean = test_label[i][test_label[i] != -100]\n",
    "\n",
    "                  predictions = logits_clean.argmax(dim=1)\n",
    "                  acc = (predictions == label_clean).float().mean()\n",
    "                  total_acc_test += acc\n",
    "        f1_metric.add_batch(predictions=predictions, references=label_clean)\n",
    "        f1 = f1_metric.compute(average='micro')\n",
    "        val_accuracy = total_acc_test / len(df_test)\n",
    "        print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
    "        print(f'f1: {f1}')\n",
    "\n",
    "\n",
    "    LEARNING_RATE = 5e-3\n",
    "    EPOCHS = max_iter #5\n",
    "    BATCH_SIZE = batch_size #5\n",
    "\n",
    "    model = BertModel()\n",
    "    train_loop(model, df_train, df_val)\n",
    "    evaluate(model, df_test)\n",
    "    # Save the model\n",
    "    model_filename = \"model.pt\"\n",
    "    model_scripted = torch.jit.script(model)\n",
    "    model_scripted.save(model_filename)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "\n",
    "    # # export json for preprocessing\n",
    "    # preprocesinng_json = {c: transformers[c].serialize_constants() \n",
    "    #                       for c in df_train.columns if c != LABEL_COLUMN}\n",
    "    # preproc_json_filename = 'preprocessing.json'\n",
    "    # with open(preproc_json_filename, 'w') as f:\n",
    "    #     json.dump(preprocesinng_json, f)\n",
    "    # gcs_preprocessing_json_path = \"{}/{}\".format(job_dir, preproc_json_filename)\n",
    "\n",
    "    # send files to GCS\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    # subprocess.check_call(['gsutil', 'cp', preproc_json_filename, gcs_preprocessing_json_path], \n",
    "    #                       stderr=sys.stdout)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69736968-1fdb-4aa8-a282-2c55048363fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## package the file into a docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f77545b-b389-4747-a6d3-07980370c3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
    "RUN pip install -U fire cloudml-hypertune pandas==0.25.3\n",
    "RUN pip install -U torchtext==0.12.0\n",
    "RUN pip install -U torch-model-archiver\n",
    "RUN pip install -U transformers\n",
    "RUN pip install -U evaluate\n",
    "ENV GPU_NUM_DEVICES=1\n",
    "ENV PJRT_DEVICE=GPU\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9512692f-b7a2-47a4-a633-319aa781fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "IMAGE_NAME = \"trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a135d707-a89c-4afc-9bbc-3da86d55d4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  11.26kB\n",
      "Step 1/11 : FROM us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
      "latest: Pulling from vertex-ai/training/pytorch-xla.1-11\n",
      "\n",
      "\u001b[1B17ec1767: Pulling fs layer \n",
      "\u001b[1B45a9c0c5: Pulling fs layer \n",
      "\u001b[1Be4c1f40f: Pulling fs layer \n",
      "\u001b[1B86ab2510: Pulling fs layer \n",
      "\u001b[1Badc78060: Pulling fs layer \n",
      "\u001b[1B84563a60: Pulling fs layer \n",
      "\u001b[1B4e23074d: Pulling fs layer \n",
      "\u001b[1Bd65705b8: Pulling fs layer \n",
      "\u001b[1Bf7b4815b: Pulling fs layer \n",
      "\u001b[1Bc0f08642: Pulling fs layer \n",
      "\u001b[1B16f81350: Pulling fs layer \n",
      "\u001b[1Bb700ef54: Pulling fs layer \n",
      "\u001b[1B60d3626c: Pulling fs layer \n",
      "\u001b[1B8c5f1f8f: Pulling fs layer \n",
      "\u001b[1B5ac89e7f: Pulling fs layer \n",
      "\u001b[1Bdcc3a763: Pulling fs layer \n",
      "\u001b[1Ba1a53c2f: Pulling fs layer \n",
      "\u001b[1Be6fcc0e6: Pulling fs layer \n",
      "\u001b[1B3226acd7: Pulling fs layer \n",
      "\u001b[1B4de043c5: Pulling fs layer \n",
      "\u001b[1B87f0858e: Pulling fs layer \n",
      "\u001b[1B07888aac: Pulling fs layer \n",
      "\u001b[1Bfc1617dd: Pulling fs layer \n",
      "\u001b[1B6205e6a8: Pulling fs layer \n",
      "\u001b[1B7f764283: Pulling fs layer \n",
      "\u001b[1Bc87c58c1: Pulling fs layer \n",
      "\u001b[1Bfd7aeafb: Pulling fs layer \n",
      "\u001b[1B2c56447b: Pulling fs layer \n",
      "\u001b[1B1d388c20: Pulling fs layer \n",
      "\u001b[1B851d47d1: Pulling fs layer \n",
      "\u001b[1B07ce84d1: Pulling fs layer \n",
      "\u001b[1Bfb1ef380: Pulling fs layer \n",
      "\u001b[1B4a355c24: Pulling fs layer \n",
      "\u001b[1B273663ee: Pulling fs layer \n",
      "\u001b[31Bdc78060: Waiting fs layer \n",
      "\u001b[31B4563a60: Waiting fs layer \n",
      "\u001b[1Bfbe58239: Pulling fs layer \n",
      "\u001b[1B8b95eefd: Pulling fs layer \n",
      "\u001b[33Be23074d: Waiting fs layer \n",
      "\u001b[1B8c98ca6d: Pulling fs layer \n",
      "\u001b[34B65705b8: Waiting fs layer \n",
      "\u001b[1B9a575097: Pulling fs layer \n",
      "\u001b[35B7b4815b: Waiting fs layer \n",
      "\u001b[1B245d6b1b: Pull complete 242kB/6.242kBB\u001b[42A\u001b[2K\u001b[43A\u001b[2K\u001b[44A\u001b[2K\u001b[41A\u001b[2K\u001b[42A\u001b[2K\u001b[38A\u001b[2K\u001b[44A\u001b[2K\u001b[37A\u001b[2K\u001b[36A\u001b[2K\u001b[37A\u001b[2K\u001b[44A\u001b[2K\u001b[35A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[44A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[44A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[43A\u001b[2K\u001b[43A\u001b[2K\u001b[35A\u001b[2K\u001b[43A\u001b[2K\u001b[39A\u001b[2K\u001b[43A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[43A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[42A\u001b[2K\u001b[39A\u001b[2K\u001b[42A\u001b[2K\u001b[39A\u001b[2K\u001b[42A\u001b[2K\u001b[39A\u001b[2K\u001b[42A\u001b[2K\u001b[37A\u001b[2K\u001b[42A\u001b[2K\u001b[42A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[40A\u001b[2K\u001b[39A\u001b[2K\u001b[40A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[37A\u001b[2K\u001b[32A\u001b[2K\u001b[37A\u001b[2K\u001b[32A\u001b[2K\u001b[37A\u001b[2K\u001b[32A\u001b[2K\u001b[37A\u001b[2K\u001b[32A\u001b[2K\u001b[37A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[31A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[31A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[32A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[30A\u001b[2K\u001b[35A\u001b[2K\u001b[30A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[31A\u001b[2K\u001b[28A\u001b[2K\u001b[31A\u001b[2K\u001b[28A\u001b[2K\u001b[31A\u001b[2K\u001b[28A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[28A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[39A\u001b[2K\u001b[28A\u001b[2K\u001b[31A\u001b[2K\u001b[39A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[39A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[39A\u001b[2K\u001b[26A\u001b[2K\u001b[39A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[39A\u001b[2K\u001b[28A\u001b[2K\u001b[22A\u001b[2K\u001b[35A\u001b[2K\u001b[21A\u001b[2K\u001b[28A\u001b[2K\u001b[20A\u001b[2K\u001b[28A\u001b[2K\u001b[39A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[35A\u001b[2K\u001b[18A\u001b[2K\u001b[35A\u001b[2K\u001b[18A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[17A\u001b[2K\u001b[35A\u001b[2K\u001b[17A\u001b[2K\u001b[28A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[28A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[16A\u001b[2K\u001b[28A\u001b[2K\u001b[16A\u001b[2K\u001b[28A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[28A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[28A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[11A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[11A\u001b[2K\u001b[35A\u001b[2K\u001b[11A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[11A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[35A\u001b[2K\u001b[11A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[39A\u001b[2K\u001b[11A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[9A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[4A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[2A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[35A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[38A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2KExtracting  403.3MB/1.23GB\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[29A\u001b[2K\u001b[29A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[27A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[25A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2KExtracting  1.462GB/3.198GB\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2KDigest: sha256:c254f30c515c0168e3481e9f3ca984fc8de698630ad966a60b9462bfd538a183\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
      " ---> 8a590d7d196b\n",
      "Step 2/11 : RUN pip install -U fire cloudml-hypertune pandas==0.25.3\n",
      " ---> Running in bc4bc849bfed\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 3.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (0.1.0.dev6)\n",
      "Collecting pandas==0.25.3\n",
      "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 75.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116951 sha256=0e9d82a6a68f0819375055264d0974ccf8f0f9e91b31a4e6bc9c6153d538e3ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, pandas, fire\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "Successfully installed fire-0.5.0 pandas-0.25.3 termcolor-2.3.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container bc4bc849bfed\n",
      " ---> fedbc8a6089d\n",
      "Step 3/11 : RUN pip install -U torchtext==0.12.0\n",
      " ---> Running in 8774aad78f5e\n",
      "Collecting torchtext==0.12.0\n",
      "  Downloading torchtext-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 47.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (1.19.5)\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (1.11.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (4.64.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0->torchtext==0.12.0) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (2.1.0)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.12.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 8774aad78f5e\n",
      " ---> fa614749b943\n",
      "Step 4/11 : RUN pip install -U torch-model-archiver\n",
      " ---> Running in 8eecb8e193fb\n",
      "Collecting torch-model-archiver\n",
      "  Downloading torch_model_archiver-0.8.1-py3-none-any.whl (14 kB)\n",
      "Collecting enum-compat\n",
      "  Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
      "Installing collected packages: enum-compat, torch-model-archiver\n",
      "Successfully installed enum-compat-0.0.3 torch-model-archiver-0.8.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 8eecb8e193fb\n",
      " ---> 42d4d0e44229\n",
      "Step 5/11 : RUN pip install -U transformers\n",
      " ---> Running in 67d85f480329\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 43.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 71.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (755 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.7/755.7 kB 58.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 34.4 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 93.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: tokenizers, safetensors, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.2 huggingface-hub-0.15.1 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 67d85f480329\n",
      " ---> 16220e8a41a5\n",
      "Step 6/11 : RUN pip install -U evaluate\n",
      " ---> Running in eb08ab629c6e\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.11.4)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.7/115.7 kB 21.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.25.3)\n",
      "Collecting datasets>=2.0.0\n",
      "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 485.6/485.6 kB 23.4 MB/s eta 0:00:00\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.1/213.1 kB 31.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2022.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.19.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
      "Successfully installed datasets-2.13.0 dill-0.3.6 evaluate-0.4.0 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container eb08ab629c6e\n",
      " ---> cee0f014d0ec\n",
      "Step 7/11 : ENV GPU_NUM_DEVICES=1\n",
      " ---> Running in 8c00236ef1b1\n",
      "Removing intermediate container 8c00236ef1b1\n",
      " ---> 9407bd457b40\n",
      "Step 8/11 : ENV PJRT_DEVICE=GPU\n",
      " ---> Running in 0610406a3668\n",
      "Removing intermediate container 0610406a3668\n",
      " ---> d7ff88da6917\n",
      "Step 9/11 : WORKDIR /app\n",
      " ---> Running in 2e459a61f2b6\n",
      "Removing intermediate container 2e459a61f2b6\n",
      " ---> 64cbb9181297\n",
      "Step 10/11 : COPY train.py .\n",
      " ---> 939dc82165ca\n",
      "Step 11/11 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 1f4efb28d7a0\n",
      "Removing intermediate container 1f4efb28d7a0\n",
      " ---> ac270d20adca\n",
      "Successfully built ac270d20adca\n",
      "Successfully tagged gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9c16f65-f1c8-447c-b4b7-f783182b2f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image:latest'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fcda289-ff34-43ba-8669-bb76f5d4a568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_app'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fca7395-a194-4a1b-84a8-482b61f077a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> -TRAINING_FILE_PATH <span style=\"color: #808000; text-decoration-color: #808000\">\"gs://qwiklabs-asl-00-69fe165840f7-ner-kfp-artifact-store/data/training/data</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">SyntaxError: </span>unexpected character after line continuation character\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m -TRAINING_FILE_PATH \u001b[33m\"gs://qwiklabs-asl-00-69fe165840f7-ner-kfp-artifact-store/data/training/data\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mSyntaxError: \u001b[0munexpected character after line continuation character\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!docker run $IMAGE_URI \\\n",
    "-job_dir \"gs://qwiklabs-asl-00-69fe165840f7-ner-kfp-artifact-store/jobs/JOB_VERTEX_20230615_154521\" \\ \n",
    "-TRAINING_FILE_PATH \"gs://qwiklabs-asl-00-69fe165840f7-ner-kfp-artifact-store/data/training/dataset.csv\" \\ \n",
    "-VALIDATION_FILE_PATH \"gs://qwiklabs-asl-00-69fe165840f7-ner-kfp-artifact-store/data/validation/dataset.csv\" \\ \n",
    "-TESTING_FILE_PATH \"gs://qwiklabs-asl-00-69fe165840f7-ner-kfp-artifact-store/data/testing/dataset.csv\" \\ \n",
    "-batch_size 2 \\ \n",
    "-max_iter 2 \\ \n",
    "-nohptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f02e3-5251-437e-8ec1-892eb1d3dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0efdad47-93ed-4e38-8070-87f81db57a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                              TAG       IMAGE ID       CREATED          SIZE\n",
      "gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image       latest    ac270d20adca   20 minutes ago   18.4GB\n",
      "us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11   latest    8a590d7d196b   4 months ago     18.2GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924631f3-980d-47e4-9e56-d805b4a659e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1b2b7a5-08d5-45ef-89d2-348ab0937488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 8.5 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://qwiklabs-asl-00-69fe165840f7_cloudbuild/source/1686842994.379759-4fa04448fd1d432b84df6686664cc5ae.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-00-69fe165840f7/locations/global/builds/e9288a1c-492c-4706-bc32-fdeacf031143].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/e9288a1c-492c-4706-bc32-fdeacf031143?project=430886062887 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e9288a1c-492c-4706-bc32-fdeacf031143\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-00-69fe165840f7_cloudbuild/source/1686842994.379759-4fa04448fd1d432b84df6686664cc5ae.tgz#1686842994640997\n",
      "Copying gs://qwiklabs-asl-00-69fe165840f7_cloudbuild/source/1686842994.379759-4fa04448fd1d432b84df6686664cc5ae.tgz#1686842994640997...\n",
      "/ [1 files][  2.7 KiB/  2.7 KiB]                                                \n",
      "Operation completed over 1 objects/2.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.26kB\n",
      "Step 1/11 : FROM us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
      "latest: Pulling from vertex-ai/training/pytorch-xla.1-11\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "602a45a9c0c5: Pulling fs layer\n",
      "e1bae4c1f40f: Pulling fs layer\n",
      "d9d586ab2510: Pulling fs layer\n",
      "2b44adc78060: Pulling fs layer\n",
      "cd4d84563a60: Pulling fs layer\n",
      "e19a4e23074d: Pulling fs layer\n",
      "a69bd65705b8: Pulling fs layer\n",
      "7145f7b4815b: Pulling fs layer\n",
      "e367c0f08642: Pulling fs layer\n",
      "256a16f81350: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "8e2860d3626c: Pulling fs layer\n",
      "3bbc8c5f1f8f: Pulling fs layer\n",
      "19fd5ac89e7f: Pulling fs layer\n",
      "6810dcc3a763: Pulling fs layer\n",
      "20f3a1a53c2f: Pulling fs layer\n",
      "0885e6fcc0e6: Pulling fs layer\n",
      "3ee43226acd7: Pulling fs layer\n",
      "607b4de043c5: Pulling fs layer\n",
      "b15287f0858e: Pulling fs layer\n",
      "507907888aac: Pulling fs layer\n",
      "700cfc1617dd: Pulling fs layer\n",
      "610d6205e6a8: Pulling fs layer\n",
      "61b77f764283: Pulling fs layer\n",
      "c891c87c58c1: Pulling fs layer\n",
      "dfb3fd7aeafb: Pulling fs layer\n",
      "ad892c56447b: Pulling fs layer\n",
      "88691d388c20: Pulling fs layer\n",
      "b007851d47d1: Pulling fs layer\n",
      "146f07ce84d1: Pulling fs layer\n",
      "4a90fb1ef380: Pulling fs layer\n",
      "02324a355c24: Pulling fs layer\n",
      "9ba4273663ee: Pulling fs layer\n",
      "213a2b0e2402: Pulling fs layer\n",
      "3cb09aa36b96: Pulling fs layer\n",
      "2723fbe58239: Pulling fs layer\n",
      "37e98b95eefd: Pulling fs layer\n",
      "9db4a00e966a: Pulling fs layer\n",
      "71268c98ca6d: Pulling fs layer\n",
      "6bf954e610e1: Pulling fs layer\n",
      "f8469a575097: Pulling fs layer\n",
      "e9dc007ff1db: Pulling fs layer\n",
      "4397245d6b1b: Pulling fs layer\n",
      "d9d586ab2510: Waiting\n",
      "2b44adc78060: Waiting\n",
      "cd4d84563a60: Waiting\n",
      "e19a4e23074d: Waiting\n",
      "a69bd65705b8: Waiting\n",
      "7145f7b4815b: Waiting\n",
      "e367c0f08642: Waiting\n",
      "256a16f81350: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "8e2860d3626c: Waiting\n",
      "3bbc8c5f1f8f: Waiting\n",
      "19fd5ac89e7f: Waiting\n",
      "6810dcc3a763: Waiting\n",
      "20f3a1a53c2f: Waiting\n",
      "0885e6fcc0e6: Waiting\n",
      "3ee43226acd7: Waiting\n",
      "607b4de043c5: Waiting\n",
      "b15287f0858e: Waiting\n",
      "507907888aac: Waiting\n",
      "700cfc1617dd: Waiting\n",
      "610d6205e6a8: Waiting\n",
      "61b77f764283: Waiting\n",
      "c891c87c58c1: Waiting\n",
      "dfb3fd7aeafb: Waiting\n",
      "ad892c56447b: Waiting\n",
      "88691d388c20: Waiting\n",
      "b007851d47d1: Waiting\n",
      "146f07ce84d1: Waiting\n",
      "4a90fb1ef380: Waiting\n",
      "02324a355c24: Waiting\n",
      "9ba4273663ee: Waiting\n",
      "213a2b0e2402: Waiting\n",
      "3cb09aa36b96: Waiting\n",
      "2723fbe58239: Waiting\n",
      "37e98b95eefd: Waiting\n",
      "9db4a00e966a: Waiting\n",
      "71268c98ca6d: Waiting\n",
      "6bf954e610e1: Waiting\n",
      "f8469a575097: Waiting\n",
      "e9dc007ff1db: Waiting\n",
      "4397245d6b1b: Waiting\n",
      "602a45a9c0c5: Verifying Checksum\n",
      "602a45a9c0c5: Download complete\n",
      "e1bae4c1f40f: Verifying Checksum\n",
      "e1bae4c1f40f: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "d9d586ab2510: Verifying Checksum\n",
      "d9d586ab2510: Download complete\n",
      "2b44adc78060: Verifying Checksum\n",
      "2b44adc78060: Download complete\n",
      "e19a4e23074d: Verifying Checksum\n",
      "e19a4e23074d: Download complete\n",
      "7145f7b4815b: Verifying Checksum\n",
      "7145f7b4815b: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "602a45a9c0c5: Pull complete\n",
      "e1bae4c1f40f: Pull complete\n",
      "d9d586ab2510: Pull complete\n",
      "2b44adc78060: Pull complete\n",
      "cd4d84563a60: Verifying Checksum\n",
      "cd4d84563a60: Download complete\n",
      "a69bd65705b8: Verifying Checksum\n",
      "a69bd65705b8: Download complete\n",
      "256a16f81350: Verifying Checksum\n",
      "256a16f81350: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "3bbc8c5f1f8f: Verifying Checksum\n",
      "3bbc8c5f1f8f: Download complete\n",
      "8e2860d3626c: Verifying Checksum\n",
      "8e2860d3626c: Download complete\n",
      "6810dcc3a763: Verifying Checksum\n",
      "6810dcc3a763: Download complete\n",
      "19fd5ac89e7f: Verifying Checksum\n",
      "19fd5ac89e7f: Download complete\n",
      "0885e6fcc0e6: Verifying Checksum\n",
      "0885e6fcc0e6: Download complete\n",
      "3ee43226acd7: Verifying Checksum\n",
      "3ee43226acd7: Download complete\n",
      "607b4de043c5: Verifying Checksum\n",
      "607b4de043c5: Download complete\n",
      "b15287f0858e: Verifying Checksum\n",
      "b15287f0858e: Download complete\n",
      "507907888aac: Verifying Checksum\n",
      "507907888aac: Download complete\n",
      "700cfc1617dd: Verifying Checksum\n",
      "700cfc1617dd: Download complete\n",
      "610d6205e6a8: Download complete\n",
      "20f3a1a53c2f: Verifying Checksum\n",
      "20f3a1a53c2f: Download complete\n",
      "61b77f764283: Verifying Checksum\n",
      "61b77f764283: Download complete\n",
      "c891c87c58c1: Verifying Checksum\n",
      "c891c87c58c1: Download complete\n",
      "dfb3fd7aeafb: Verifying Checksum\n",
      "dfb3fd7aeafb: Download complete\n",
      "ad892c56447b: Verifying Checksum\n",
      "ad892c56447b: Download complete\n",
      "b007851d47d1: Verifying Checksum\n",
      "b007851d47d1: Download complete\n",
      "146f07ce84d1: Verifying Checksum\n",
      "146f07ce84d1: Download complete\n",
      "4a90fb1ef380: Verifying Checksum\n",
      "4a90fb1ef380: Download complete\n",
      "02324a355c24: Verifying Checksum\n",
      "02324a355c24: Download complete\n",
      "9ba4273663ee: Verifying Checksum\n",
      "9ba4273663ee: Download complete\n",
      "213a2b0e2402: Verifying Checksum\n",
      "213a2b0e2402: Download complete\n",
      "3cb09aa36b96: Verifying Checksum\n",
      "3cb09aa36b96: Download complete\n",
      "2723fbe58239: Verifying Checksum\n",
      "2723fbe58239: Download complete\n",
      "37e98b95eefd: Download complete\n",
      "9db4a00e966a: Verifying Checksum\n",
      "9db4a00e966a: Download complete\n",
      "71268c98ca6d: Verifying Checksum\n",
      "71268c98ca6d: Download complete\n",
      "6bf954e610e1: Verifying Checksum\n",
      "6bf954e610e1: Download complete\n",
      "f8469a575097: Verifying Checksum\n",
      "f8469a575097: Download complete\n",
      "e9dc007ff1db: Verifying Checksum\n",
      "e9dc007ff1db: Download complete\n",
      "4397245d6b1b: Verifying Checksum\n",
      "4397245d6b1b: Download complete\n",
      "e367c0f08642: Verifying Checksum\n",
      "e367c0f08642: Download complete\n",
      "cd4d84563a60: Pull complete\n",
      "e19a4e23074d: Pull complete\n",
      "88691d388c20: Verifying Checksum\n",
      "88691d388c20: Download complete\n",
      "a69bd65705b8: Pull complete\n",
      "7145f7b4815b: Pull complete\n",
      "e367c0f08642: Pull complete\n",
      "256a16f81350: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "8e2860d3626c: Pull complete\n",
      "3bbc8c5f1f8f: Pull complete\n",
      "19fd5ac89e7f: Pull complete\n",
      "6810dcc3a763: Pull complete\n",
      "20f3a1a53c2f: Pull complete\n",
      "0885e6fcc0e6: Pull complete\n",
      "3ee43226acd7: Pull complete\n",
      "607b4de043c5: Pull complete\n",
      "b15287f0858e: Pull complete\n",
      "507907888aac: Pull complete\n",
      "700cfc1617dd: Pull complete\n",
      "610d6205e6a8: Pull complete\n",
      "61b77f764283: Pull complete\n",
      "c891c87c58c1: Pull complete\n",
      "dfb3fd7aeafb: Pull complete\n",
      "ad892c56447b: Pull complete\n",
      "88691d388c20: Pull complete\n",
      "b007851d47d1: Pull complete\n",
      "146f07ce84d1: Pull complete\n",
      "4a90fb1ef380: Pull complete\n",
      "02324a355c24: Pull complete\n",
      "9ba4273663ee: Pull complete\n",
      "213a2b0e2402: Pull complete\n",
      "3cb09aa36b96: Pull complete\n",
      "2723fbe58239: Pull complete\n",
      "37e98b95eefd: Pull complete\n",
      "9db4a00e966a: Pull complete\n",
      "71268c98ca6d: Pull complete\n",
      "6bf954e610e1: Pull complete\n",
      "f8469a575097: Pull complete\n",
      "e9dc007ff1db: Pull complete\n",
      "4397245d6b1b: Pull complete\n",
      "Digest: sha256:c254f30c515c0168e3481e9f3ca984fc8de698630ad966a60b9462bfd538a183\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
      " ---> 8a590d7d196b\n",
      "Step 2/11 : RUN pip install -U fire cloudml-hypertune pandas==0.25.3\n",
      " ---> Running in eeb301eb586c\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 7.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (0.1.0.dev6)\n",
      "Collecting pandas==0.25.3\n",
      "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 33.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116951 sha256=6e45c629a146fa91cb45434bc70107105e6653de446c0c9b4edaf1a7fedbe5dd\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, pandas, fire\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "Successfully installed fire-0.5.0 pandas-0.25.3 termcolor-2.3.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container eeb301eb586c\n",
      " ---> 1d0268cf3218\n",
      "Step 3/11 : RUN pip install -U torchtext==0.12.0\n",
      " ---> Running in bf599513b503\n",
      "Collecting torchtext==0.12.0\n",
      "  Downloading torchtext-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (4.64.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (2.28.1)\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0->torchtext==0.12.0) (4.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (3.3)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.12.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container bf599513b503\n",
      " ---> 348332af8b0b\n",
      "Step 4/11 : RUN pip install -U torch-model-archiver\n",
      " ---> Running in 984b33b883f0\n",
      "Collecting torch-model-archiver\n",
      "  Downloading torch_model_archiver-0.8.1-py3-none-any.whl (14 kB)\n",
      "Collecting enum-compat\n",
      "  Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
      "Installing collected packages: enum-compat, torch-model-archiver\n",
      "Successfully installed enum-compat-0.0.3 torch-model-archiver-0.8.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 984b33b883f0\n",
      " ---> fa340bdeea53\n",
      "Step 5/11 : RUN pip install -U transformers\n",
      " ---> Running in 802eeef924cd\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 27.6 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 51.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 24.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (755 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.7/755.7 kB 43.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 17.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.2.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Installing collected packages: tokenizers, safetensors, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.2 huggingface-hub-0.15.1 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 802eeef924cd\n",
      " ---> 82ecfd8898e9\n",
      "Step 6/11 : RUN pip install -U evaluate\n",
      " ---> Running in 0e0fd8e2cd86\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 8.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.25.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.11.4)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.7/115.7 kB 14.7 MB/s eta 0:00:00\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 15.3 MB/s eta 0:00:00\n",
      "Collecting datasets>=2.0.0\n",
      "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 485.6/485.6 kB 34.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.19.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.1/213.1 kB 26.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2022.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
      "Successfully installed datasets-2.13.0 dill-0.3.6 evaluate-0.4.0 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 0e0fd8e2cd86\n",
      " ---> d6e48618ef2f\n",
      "Step 7/11 : ENV GPU_NUM_DEVICES=1\n",
      " ---> Running in 5b1ef771a67e\n",
      "Removing intermediate container 5b1ef771a67e\n",
      " ---> 9dccafe10e9c\n",
      "Step 8/11 : ENV PJRT_DEVICE=GPU\n",
      " ---> Running in e3cef1ce5ad0\n",
      "Removing intermediate container e3cef1ce5ad0\n",
      " ---> 80bc086aa996\n",
      "Step 9/11 : WORKDIR /app\n",
      " ---> Running in 8a044a762097\n",
      "Removing intermediate container 8a044a762097\n",
      " ---> 5c12c25af502\n",
      "Step 10/11 : COPY train.py .\n",
      " ---> 08ea1663f2cd\n",
      "Step 11/11 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 0f23859f6261\n",
      "Removing intermediate container 0f23859f6261\n",
      " ---> e159e8afd91f\n",
      "Successfully built e159e8afd91f\n",
      "Successfully tagged gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image]\n",
      "c65f1b75b479: Preparing\n",
      "4a6bf91dd199: Preparing\n",
      "31ceee06cb59: Preparing\n",
      "a2fde3bfffd3: Preparing\n",
      "c4e571795e97: Preparing\n",
      "763f200e9acc: Preparing\n",
      "77ef3ea2212c: Preparing\n",
      "d0f503eb5103: Preparing\n",
      "2090105d38de: Preparing\n",
      "e74f6867eca5: Preparing\n",
      "c0cceb8da3b6: Preparing\n",
      "f9dd184ec603: Preparing\n",
      "4aee62086b60: Preparing\n",
      "1bd06172a32a: Preparing\n",
      "3e584b235170: Preparing\n",
      "87c655d0c002: Preparing\n",
      "468113a3db80: Preparing\n",
      "aa1bb6b42754: Preparing\n",
      "aa7c8303dfba: Preparing\n",
      "b53563712287: Preparing\n",
      "b53563712287: Preparing\n",
      "fc1fb2554a41: Preparing\n",
      "a2fc4cabb51a: Preparing\n",
      "e8b52ca691f4: Preparing\n",
      "e4d4d528cf5d: Preparing\n",
      "25ace76efa07: Preparing\n",
      "e69159dfa907: Preparing\n",
      "ce2f668df2d8: Preparing\n",
      "1c26767a76ae: Preparing\n",
      "7e2f559b3e11: Preparing\n",
      "85af48d15b8f: Preparing\n",
      "512412b9f6a5: Preparing\n",
      "f0418bde03c7: Preparing\n",
      "97800cee6822: Preparing\n",
      "98095ac9575b: Preparing\n",
      "c4ebf9d6e076: Preparing\n",
      "440df9268ba0: Preparing\n",
      "a1f6b1a8fb1a: Preparing\n",
      "9ae92ce49008: Preparing\n",
      "6d8137497d3c: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "20634b178955: Preparing\n",
      "1a5fac543081: Preparing\n",
      "a8d0c4c62eef: Preparing\n",
      "7ed9a71261c7: Preparing\n",
      "a1eeba43cdbe: Preparing\n",
      "6127942867a5: Preparing\n",
      "e592fe6d10a9: Preparing\n",
      "f42691182163: Preparing\n",
      "68016c5bb65c: Preparing\n",
      "8034550a3bbe: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "763f200e9acc: Waiting\n",
      "77ef3ea2212c: Waiting\n",
      "d0f503eb5103: Waiting\n",
      "2090105d38de: Waiting\n",
      "e74f6867eca5: Waiting\n",
      "c0cceb8da3b6: Waiting\n",
      "f9dd184ec603: Waiting\n",
      "4aee62086b60: Waiting\n",
      "1bd06172a32a: Waiting\n",
      "3e584b235170: Waiting\n",
      "87c655d0c002: Waiting\n",
      "468113a3db80: Waiting\n",
      "aa1bb6b42754: Waiting\n",
      "aa7c8303dfba: Waiting\n",
      "b53563712287: Waiting\n",
      "fc1fb2554a41: Waiting\n",
      "a2fc4cabb51a: Waiting\n",
      "e8b52ca691f4: Waiting\n",
      "e4d4d528cf5d: Waiting\n",
      "25ace76efa07: Waiting\n",
      "e69159dfa907: Waiting\n",
      "ce2f668df2d8: Waiting\n",
      "1c26767a76ae: Waiting\n",
      "7e2f559b3e11: Waiting\n",
      "85af48d15b8f: Waiting\n",
      "512412b9f6a5: Waiting\n",
      "f0418bde03c7: Waiting\n",
      "97800cee6822: Waiting\n",
      "98095ac9575b: Waiting\n",
      "c4ebf9d6e076: Waiting\n",
      "440df9268ba0: Waiting\n",
      "a1f6b1a8fb1a: Waiting\n",
      "9ae92ce49008: Waiting\n",
      "6d8137497d3c: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "20634b178955: Waiting\n",
      "1a5fac543081: Waiting\n",
      "a8d0c4c62eef: Waiting\n",
      "7ed9a71261c7: Waiting\n",
      "a1eeba43cdbe: Waiting\n",
      "6127942867a5: Waiting\n",
      "e592fe6d10a9: Waiting\n",
      "f42691182163: Waiting\n",
      "68016c5bb65c: Waiting\n",
      "8034550a3bbe: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "c4e571795e97: Pushed\n",
      "c65f1b75b479: Pushed\n",
      "4a6bf91dd199: Pushed\n",
      "d0f503eb5103: Layer already exists\n",
      "2090105d38de: Layer already exists\n",
      "e74f6867eca5: Layer already exists\n",
      "c0cceb8da3b6: Layer already exists\n",
      "31ceee06cb59: Pushed\n",
      "f9dd184ec603: Layer already exists\n",
      "4aee62086b60: Layer already exists\n",
      "1bd06172a32a: Layer already exists\n",
      "3e584b235170: Layer already exists\n",
      "87c655d0c002: Layer already exists\n",
      "aa1bb6b42754: Layer already exists\n",
      "468113a3db80: Layer already exists\n",
      "aa7c8303dfba: Layer already exists\n",
      "b53563712287: Layer already exists\n",
      "fc1fb2554a41: Layer already exists\n",
      "a2fc4cabb51a: Layer already exists\n",
      "e8b52ca691f4: Layer already exists\n",
      "e4d4d528cf5d: Layer already exists\n",
      "25ace76efa07: Layer already exists\n",
      "e69159dfa907: Layer already exists\n",
      "ce2f668df2d8: Layer already exists\n",
      "1c26767a76ae: Layer already exists\n",
      "7e2f559b3e11: Layer already exists\n",
      "85af48d15b8f: Layer already exists\n",
      "f0418bde03c7: Layer already exists\n",
      "512412b9f6a5: Layer already exists\n",
      "97800cee6822: Layer already exists\n",
      "98095ac9575b: Layer already exists\n",
      "c4ebf9d6e076: Layer already exists\n",
      "440df9268ba0: Layer already exists\n",
      "a1f6b1a8fb1a: Layer already exists\n",
      "9ae92ce49008: Layer already exists\n",
      "6d8137497d3c: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "20634b178955: Layer already exists\n",
      "1a5fac543081: Layer already exists\n",
      "763f200e9acc: Pushed\n",
      "a8d0c4c62eef: Layer already exists\n",
      "7ed9a71261c7: Layer already exists\n",
      "a1eeba43cdbe: Layer already exists\n",
      "6127942867a5: Layer already exists\n",
      "f42691182163: Layer already exists\n",
      "e592fe6d10a9: Layer already exists\n",
      "68016c5bb65c: Layer already exists\n",
      "8034550a3bbe: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "a2fde3bfffd3: Pushed\n",
      "77ef3ea2212c: Pushed\n",
      "latest: digest: sha256:a36444e3b25812db41853d3d755c828dddbaa08892e0ca69f5ea8e863d9791c1 size: 11203\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "e9288a1c-492c-4706-bc32-fdeacf031143  2023-06-15T15:29:54+00:00  12M4S     gs://qwiklabs-asl-00-69fe165840f7_cloudbuild/source/1686842994.379759-4fa04448fd1d432b84df6686664cc5ae.tgz  gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0ecc8e8-2ae5-4d68-b036-bbe7ea7cef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1387352-9a75-4d26-92cf-c4ce54427495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 5\n",
    "# max_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce560154-08bf-4cbd-891f-b6c97cf987a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/430886062887/locations/us-central1/customJobs/6826300211422298112] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/430886062887/locations/us-central1/customJobs/6826300211422298112\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/430886062887/locations/us-central1/customJobs/6826300211422298112\n",
      "The model will be exported at: gs://qwiklabs-asl-00-69fe165840f7-ner-kfp-artifact-store/jobs/JOB_VERTEX_20230615_154521\n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"JOB_VERTEX_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\n",
    "ACCELERATOR_COUNT=1\n",
    "\n",
    "WORKER_POOL_SPEC = f\"\"\"\\\n",
    "machine-type={MACHINE_TYPE},\\\n",
    "replica-count={REPLICA_COUNT},\\\n",
    "accelerator-type={ACCELERATOR_TYPE},\\\n",
    "accelerator-count={ACCELERATOR_COUNT},\\\n",
    "container-image-uri={IMAGE_URI}\\\n",
    "\"\"\"\n",
    "\n",
    "ARGS = f\"\"\"\\\n",
    "--job_dir={JOB_DIR},\\\n",
    "--TRAINING_FILE_PATH={TRAINING_FILE_PATH},\\\n",
    "--VALIDATION_FILE_PATH={VALIDATION_FILE_PATH},\\\n",
    "--TESTING_FILE_PATH={TESTING_FILE_PATH},\\\n",
    "--batch_size=2,\\\n",
    "--max_iter=2,\\\n",
    "--nohptune\\\n",
    "\"\"\"\n",
    "\n",
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "  --args={ARGS}\n",
    "\n",
    "print(\"The model will be exported at:\", JOB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f082f9ae-6b6b-4ff1-baaf-3c647e843ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77b05b05-815b-4b77-8e76-e19dc4c630e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python3: Error while finding module specification for '/training_app/train.py' (ModuleNotFoundError: No module named '/training_app/train'). Try using '/training_app/train' instead of '/training_app/train.py' as the module name.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 get_ipython().run_cell_magic(<span style=\"color: #808000; text-decoration-color: #808000\">'bash'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">''</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'python3 -m /training_app/train.py --job_dir $J</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/IPython/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">interactiveshell.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2478</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_cell_magic</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2475 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2476 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.builtin_trap:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2477 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>args = (magic_arg_s, cell)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2478 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>result = fn(*args, **kwargs)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2479 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2480 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The code below prevents the output from being displayed</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2481 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># when using magics with decodator @output_can_be_silenced</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/IPython/core/magics/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">script.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">154</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">named_script_magic</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>line = <span style=\"color: #808000; text-decoration-color: #808000\">\"%s %s\"</span> % (script, line)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">153 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>line = script                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>154 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.shebang(line, cell)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># write a basic docstring:</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">157 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>named_script_magic.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__doc__</span> = \\                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/IPython/core/magics/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">script.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">314</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">shebang</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">311 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># killed it but not yet seen its return code. We don't wait for it,</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">312 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># in case it's stuck in uninterruptible sleep. -9 = SIGKILL</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">313 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>rc = p.returncode <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>314 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> CalledProcessError(rc, cell)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">316 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>shebang.__skip_doctest__ = os.name != <span style=\"color: #808000; text-decoration-color: #808000\">\"posix\"</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">CalledProcessError: </span>Command <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>python3 -m <span style=\"color: #800080; text-decoration-color: #800080\">/training_app/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">train.py</span> --job_dir $JOB_DIR, \\\\\\n--TRAINING_FILE_PATH \n",
       "$TRAINING_FILE_PATH, \\\\\\n--VALIDATION_FILE_PATH $VALIDATION_FILE_PATH, \\\\\\n--TESTING_FILE_PATH $TESTING_FILE_PATH, \n",
       "\\\\\\n--batch_size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, \\\\\\n--max_iter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, \\\\\\n--hptune <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> \\\\\\n'' returned non-zero exit status <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 get_ipython().run_cell_magic(\u001b[33m'\u001b[0m\u001b[33mbash\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mpython3 -m /training_app/train.py --job_dir $J\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/IPython/core/\u001b[0m\u001b[1;33minteractiveshell.py\u001b[0m:\u001b[94m2478\u001b[0m in \u001b[92mrun_cell_magic\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2475 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2476 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.builtin_trap:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2477 \u001b[0m\u001b[2m│   │   │   │   \u001b[0margs = (magic_arg_s, cell)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2478 \u001b[2m│   │   │   │   \u001b[0mresult = fn(*args, **kwargs)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2479 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2480 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The code below prevents the output from being displayed\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2481 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# when using magics with decodator @output_can_be_silenced\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/\u001b[0m\u001b[1;33mscript.py\u001b[0m:\u001b[94m154\u001b[0m in \u001b[92mnamed_script_magic\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mline = \u001b[33m\"\u001b[0m\u001b[33m%s\u001b[0m\u001b[33m \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m % (script, line)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mline = script                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m154 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.shebang(line, cell)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# write a basic docstring:\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   │   \u001b[0mnamed_script_magic.\u001b[91m__doc__\u001b[0m = \\                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/\u001b[0m\u001b[1;33mscript.py\u001b[0m:\u001b[94m314\u001b[0m in \u001b[92mshebang\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m311 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# killed it but not yet seen its return code. We don't wait for it,\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m312 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   │   │   \u001b[0mrc = p.returncode \u001b[95mor\u001b[0m -\u001b[94m9\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m314 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m CalledProcessError(rc, cell)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   \u001b[0mshebang.__skip_doctest__ = os.name != \u001b[33m\"\u001b[0m\u001b[33mposix\u001b[0m\u001b[33m\"\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m317 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mCalledProcessError: \u001b[0mCommand \u001b[32m'b'\u001b[0mpython3 -m \u001b[35m/training_app/\u001b[0m\u001b[95mtrain.py\u001b[0m --job_dir $JOB_DIR, \\\\\\n--TRAINING_FILE_PATH \n",
       "$TRAINING_FILE_PATH, \\\\\\n--VALIDATION_FILE_PATH $VALIDATION_FILE_PATH, \\\\\\n--TESTING_FILE_PATH $TESTING_FILE_PATH, \n",
       "\\\\\\n--batch_size \u001b[1;36m2\u001b[0m, \\\\\\n--max_iter \u001b[1;36m2\u001b[0m, \\\\\\n--hptune \u001b[3;91mFalse\u001b[0m \\\\\\n'' returned non-zero exit status \u001b[1;36m1\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m /training_app/train.py --job_dir $JOB_DIR, \\\n",
    "--TRAINING_FILE_PATH $TRAINING_FILE_PATH, \\\n",
    "--VALIDATION_FILE_PATH $VALIDATION_FILE_PATH, \\\n",
    "--TESTING_FILE_PATH $TESTING_FILE_PATH, \\\n",
    "--batch_size 2, \\\n",
    "--max_iter 2, \\\n",
    "--hptune False \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6177ad-3007-4657-a1b2-19188df0a915",
   "metadata": {},
   "source": [
    "## Deploy model to vertexAI prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21bae9-41c3-4519-9ddf-0ede353a4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_APP_FOLDER = \"predict_app\"\n",
    "os.makedirs(PREDICT_APP_FOLDER, exist_ok=True)\n",
    "os.environ[\"PREDICT_APP_FOLDER\"] = PREDICT_APP_FOLDER\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0dfc19-81da-4491-a43b-9aee494c3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PREDICT_APP_FOLDER}/handler.py\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchtext.vocab import vocab\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63c72e-d862-48e1-9dca-56623dabc7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d18b58-42e6-4667-a216-020bde47cd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4a607-f9c4-42a6-9b36-2ae5caab3c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c196c-3655-4269-9bda-979b80d9b7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ea4c7-cb94-4b2d-94e6-454bfb23ee42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57466b5-c30c-4248-91b5-772de38ad159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7777c6b1-914c-428f-a3f5-b6be1f221f18",
   "metadata": {},
   "source": [
    "## ============= Below is the code from Takumi's template; to be deleted ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f30146ae-d512-4f0a-9063-dc19586d9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "CATEGORICAL_FEATURES = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "NUMERICAL_FEATURES = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "LABEL_COLUMN = \"Cover_Type\"\n",
    "\n",
    "\n",
    "class OneHotEncoder:\n",
    "    def fit(self, series: pd.Series):\n",
    "        categories = series.unique().tolist()\n",
    "        dictionary = vocab(OrderedDict([(cat, 1) for cat in categories]))\n",
    "        self.dictionary = dictionary\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature:list, dictionary=None) -> torch.Tensor:\n",
    "        if dictionary:\n",
    "            assert type(dictionary) == list\n",
    "            self.dictionary = vocab(\n",
    "                OrderedDict([(cat, 1) for cat in dictionary])\n",
    "            )\n",
    "\n",
    "        indices = self.dictionary.lookup_indices(feature)\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        one_hot = F.one_hot(\n",
    "            indices, num_classes=len(self.dictionary.get_itos())\n",
    "        )\n",
    "        return torch.tensor(one_hot).float()\n",
    "\n",
    "    def serialize_constants(self):\n",
    "        return {\"dictionary\": self.dictionary.get_itos()}\n",
    "class StandardScaler:\n",
    "    def fit(self, series: pd.Series):\n",
    "        self.mean = np.float64(series.mean())\n",
    "        self.std = np.float64(series.std())\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature:list, mean=None, std=None) -> torch.Tensor:\n",
    "        if mean:\n",
    "            self.mean = np.float64(mean)\n",
    "        if std:\n",
    "            self.std = np.float64(std)\n",
    "\n",
    "        standardized = (feature - self.mean) / self.std\n",
    "        return torch.tensor(standardized)[:, None].float()\n",
    "\n",
    "    def serialize_constants(self):\n",
    "        return {\"mean\": self.mean, \"std\": self.std}\n",
    "\n",
    "def preprocess(df, transformers):\n",
    "    transformed_features = [transformers[c].transform(df[c].to_list()) \n",
    "                            for c in df.columns if c != LABEL_COLUMN]\n",
    "    features = torch.cat(transformed_features, 1)\n",
    "    \n",
    "    label = df[LABEL_COLUMN].to_list()\n",
    "    label = torch.LongTensor(label)\n",
    "    return features, label\n",
    "\n",
    "    \n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, batch_size, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    \n",
    "    # Preproc Categorical columns\n",
    "    transformers = {c_feature: OneHotEncoder().fit(df_train[c_feature]) \n",
    "                    for c_feature in CATEGORICAL_FEATURES}\n",
    "    transformers.update({n_feature: StandardScaler().fit(df_train[n_feature]) \n",
    "                         for n_feature in NUMERICAL_FEATURES})\n",
    "\n",
    "    training_transformed = preprocess(df_train, transformers)\n",
    "    validation_transformed = preprocess(df_validation, transformers)\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = TensorDataset(*training_transformed)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    validation_dataset = TensorDataset(*validation_transformed)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "        # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    class NeuralNetworkModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(54, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 7)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.linear_relu_stack(x)\n",
    "\n",
    "    model = NeuralNetworkModel().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train(dataloader, model, loss_fn, optimizer, device):\n",
    "        model.train()\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def validation(dataloader, model, loss_fn, device):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        validation_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                validation_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        validation_loss /= num_batches\n",
    "        correct /= size\n",
    "        return correct, validation_loss\n",
    "    \n",
    "    epochs = max_iter\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "\n",
    "    if hptune:\n",
    "        accuracy, _ = validation(validation_dataloader, model, loss_fn, device)\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    if not hptune:\n",
    "        # Save the model\n",
    "        model_filename = \"model.pt\"\n",
    "        model_scripted = torch.jit.script(model)\n",
    "        model_scripted.save(model_filename)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "\n",
    "        # export json for preprocessing\n",
    "        preprocesinng_json = {c: transformers[c].serialize_constants() \n",
    "                              for c in df_train.columns if c != LABEL_COLUMN}\n",
    "        preproc_json_filename = 'preprocessing.json'\n",
    "        with open(preproc_json_filename, 'w') as f:\n",
    "            json.dump(preprocesinng_json, f)\n",
    "        gcs_preprocessing_json_path = \"{}/{}\".format(job_dir, preproc_json_filename)\n",
    "\n",
    "        # send files to GCS\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        subprocess.check_call(['gsutil', 'cp', preproc_json_filename, gcs_preprocessing_json_path], \n",
    "                              stderr=sys.stdout)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34174242-7a98-4251-93d2-1dfd55639423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
    "RUN pip install -U fire cloudml-hypertune pandas==0.25.3\n",
    "RUN pip install -U torchtext==0.12.0\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b82a5d0-d080-4eb1-bfd5-ba9d7f925b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba0d0531-aafa-4ed3-85a2-e80ba5c9e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 6.6 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://qwiklabs-asl-00-69fe165840f7_cloudbuild/source/1686596468.642348-67fef37a619f455cb4f9d3b03137571c.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-00-69fe165840f7/locations/global/builds/931d9ff3-f4ff-4186-b4a3-28165f912df9].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/931d9ff3-f4ff-4186-b4a3-28165f912df9?project=430886062887 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"931d9ff3-f4ff-4186-b4a3-28165f912df9\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-00-69fe165840f7_cloudbuild/source/1686596468.642348-67fef37a619f455cb4f9d3b03137571c.tgz#1686596468916955\n",
      "Copying gs://qwiklabs-asl-00-69fe165840f7_cloudbuild/source/1686596468.642348-67fef37a619f455cb4f9d3b03137571c.tgz#1686596468916955...\n",
      "/ [1 files][  2.5 KiB/  2.5 KiB]                                                \n",
      "Operation completed over 1 objects/2.5 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  9.216kB\n",
      "Step 1/6 : FROM us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
      "latest: Pulling from vertex-ai/training/pytorch-xla.1-11\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "602a45a9c0c5: Pulling fs layer\n",
      "e1bae4c1f40f: Pulling fs layer\n",
      "d9d586ab2510: Pulling fs layer\n",
      "2b44adc78060: Pulling fs layer\n",
      "cd4d84563a60: Pulling fs layer\n",
      "e19a4e23074d: Pulling fs layer\n",
      "a69bd65705b8: Pulling fs layer\n",
      "7145f7b4815b: Pulling fs layer\n",
      "e367c0f08642: Pulling fs layer\n",
      "256a16f81350: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "8e2860d3626c: Pulling fs layer\n",
      "3bbc8c5f1f8f: Pulling fs layer\n",
      "19fd5ac89e7f: Pulling fs layer\n",
      "6810dcc3a763: Pulling fs layer\n",
      "20f3a1a53c2f: Pulling fs layer\n",
      "0885e6fcc0e6: Pulling fs layer\n",
      "3ee43226acd7: Pulling fs layer\n",
      "607b4de043c5: Pulling fs layer\n",
      "b15287f0858e: Pulling fs layer\n",
      "507907888aac: Pulling fs layer\n",
      "700cfc1617dd: Pulling fs layer\n",
      "610d6205e6a8: Pulling fs layer\n",
      "61b77f764283: Pulling fs layer\n",
      "c891c87c58c1: Pulling fs layer\n",
      "dfb3fd7aeafb: Pulling fs layer\n",
      "ad892c56447b: Pulling fs layer\n",
      "88691d388c20: Pulling fs layer\n",
      "b007851d47d1: Pulling fs layer\n",
      "146f07ce84d1: Pulling fs layer\n",
      "4a90fb1ef380: Pulling fs layer\n",
      "d9d586ab2510: Waiting\n",
      "02324a355c24: Pulling fs layer\n",
      "9ba4273663ee: Pulling fs layer\n",
      "213a2b0e2402: Pulling fs layer\n",
      "3cb09aa36b96: Pulling fs layer\n",
      "2723fbe58239: Pulling fs layer\n",
      "37e98b95eefd: Pulling fs layer\n",
      "9db4a00e966a: Pulling fs layer\n",
      "71268c98ca6d: Pulling fs layer\n",
      "6bf954e610e1: Pulling fs layer\n",
      "f8469a575097: Pulling fs layer\n",
      "e9dc007ff1db: Pulling fs layer\n",
      "4397245d6b1b: Pulling fs layer\n",
      "20f3a1a53c2f: Waiting\n",
      "0885e6fcc0e6: Waiting\n",
      "3ee43226acd7: Waiting\n",
      "607b4de043c5: Waiting\n",
      "b15287f0858e: Waiting\n",
      "507907888aac: Waiting\n",
      "700cfc1617dd: Waiting\n",
      "610d6205e6a8: Waiting\n",
      "61b77f764283: Waiting\n",
      "c891c87c58c1: Waiting\n",
      "dfb3fd7aeafb: Waiting\n",
      "ad892c56447b: Waiting\n",
      "88691d388c20: Waiting\n",
      "b007851d47d1: Waiting\n",
      "146f07ce84d1: Waiting\n",
      "4a90fb1ef380: Waiting\n",
      "02324a355c24: Waiting\n",
      "9ba4273663ee: Waiting\n",
      "213a2b0e2402: Waiting\n",
      "3cb09aa36b96: Waiting\n",
      "2723fbe58239: Waiting\n",
      "37e98b95eefd: Waiting\n",
      "9db4a00e966a: Waiting\n",
      "71268c98ca6d: Waiting\n",
      "6bf954e610e1: Waiting\n",
      "f8469a575097: Waiting\n",
      "e9dc007ff1db: Waiting\n",
      "4397245d6b1b: Waiting\n",
      "2b44adc78060: Waiting\n",
      "cd4d84563a60: Waiting\n",
      "e19a4e23074d: Waiting\n",
      "a69bd65705b8: Waiting\n",
      "7145f7b4815b: Waiting\n",
      "e367c0f08642: Waiting\n",
      "256a16f81350: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "8e2860d3626c: Waiting\n",
      "3bbc8c5f1f8f: Waiting\n",
      "19fd5ac89e7f: Waiting\n",
      "6810dcc3a763: Waiting\n",
      "e1bae4c1f40f: Verifying Checksum\n",
      "e1bae4c1f40f: Download complete\n",
      "602a45a9c0c5: Verifying Checksum\n",
      "602a45a9c0c5: Download complete\n",
      "d5fd17ec1767: Download complete\n",
      "d9d586ab2510: Verifying Checksum\n",
      "d9d586ab2510: Download complete\n",
      "2b44adc78060: Verifying Checksum\n",
      "2b44adc78060: Download complete\n",
      "e19a4e23074d: Verifying Checksum\n",
      "e19a4e23074d: Download complete\n",
      "7145f7b4815b: Verifying Checksum\n",
      "7145f7b4815b: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "602a45a9c0c5: Pull complete\n",
      "cd4d84563a60: Download complete\n",
      "e1bae4c1f40f: Pull complete\n",
      "256a16f81350: Verifying Checksum\n",
      "256a16f81350: Download complete\n",
      "d9d586ab2510: Pull complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "2b44adc78060: Pull complete\n",
      "a69bd65705b8: Verifying Checksum\n",
      "a69bd65705b8: Download complete\n",
      "8e2860d3626c: Verifying Checksum\n",
      "8e2860d3626c: Download complete\n",
      "19fd5ac89e7f: Verifying Checksum\n",
      "19fd5ac89e7f: Download complete\n",
      "6810dcc3a763: Verifying Checksum\n",
      "6810dcc3a763: Download complete\n",
      "3bbc8c5f1f8f: Verifying Checksum\n",
      "3bbc8c5f1f8f: Download complete\n",
      "0885e6fcc0e6: Download complete\n",
      "3ee43226acd7: Verifying Checksum\n",
      "3ee43226acd7: Download complete\n",
      "607b4de043c5: Verifying Checksum\n",
      "607b4de043c5: Download complete\n",
      "b15287f0858e: Verifying Checksum\n",
      "b15287f0858e: Download complete\n",
      "507907888aac: Verifying Checksum\n",
      "507907888aac: Download complete\n",
      "20f3a1a53c2f: Verifying Checksum\n",
      "20f3a1a53c2f: Download complete\n",
      "610d6205e6a8: Verifying Checksum\n",
      "610d6205e6a8: Download complete\n",
      "700cfc1617dd: Verifying Checksum\n",
      "700cfc1617dd: Download complete\n",
      "c891c87c58c1: Verifying Checksum\n",
      "c891c87c58c1: Download complete\n",
      "61b77f764283: Verifying Checksum\n",
      "61b77f764283: Download complete\n",
      "ad892c56447b: Download complete\n",
      "dfb3fd7aeafb: Verifying Checksum\n",
      "dfb3fd7aeafb: Download complete\n",
      "b007851d47d1: Verifying Checksum\n",
      "b007851d47d1: Download complete\n",
      "146f07ce84d1: Verifying Checksum\n",
      "146f07ce84d1: Download complete\n",
      "4a90fb1ef380: Download complete\n",
      "02324a355c24: Verifying Checksum\n",
      "02324a355c24: Download complete\n",
      "e367c0f08642: Download complete\n",
      "213a2b0e2402: Verifying Checksum\n",
      "213a2b0e2402: Download complete\n",
      "3cb09aa36b96: Verifying Checksum\n",
      "3cb09aa36b96: Download complete\n",
      "2723fbe58239: Verifying Checksum\n",
      "2723fbe58239: Download complete\n",
      "37e98b95eefd: Verifying Checksum\n",
      "37e98b95eefd: Download complete\n",
      "9db4a00e966a: Verifying Checksum\n",
      "9db4a00e966a: Download complete\n",
      "71268c98ca6d: Verifying Checksum\n",
      "71268c98ca6d: Download complete\n",
      "6bf954e610e1: Verifying Checksum\n",
      "6bf954e610e1: Download complete\n",
      "9ba4273663ee: Verifying Checksum\n",
      "9ba4273663ee: Download complete\n",
      "f8469a575097: Verifying Checksum\n",
      "f8469a575097: Download complete\n",
      "e9dc007ff1db: Verifying Checksum\n",
      "e9dc007ff1db: Download complete\n",
      "4397245d6b1b: Verifying Checksum\n",
      "4397245d6b1b: Download complete\n",
      "cd4d84563a60: Pull complete\n",
      "e19a4e23074d: Pull complete\n",
      "88691d388c20: Verifying Checksum\n",
      "88691d388c20: Download complete\n",
      "a69bd65705b8: Pull complete\n",
      "7145f7b4815b: Pull complete\n",
      "e367c0f08642: Pull complete\n",
      "256a16f81350: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "8e2860d3626c: Pull complete\n",
      "3bbc8c5f1f8f: Pull complete\n",
      "19fd5ac89e7f: Pull complete\n",
      "6810dcc3a763: Pull complete\n",
      "20f3a1a53c2f: Pull complete\n",
      "0885e6fcc0e6: Pull complete\n",
      "3ee43226acd7: Pull complete\n",
      "607b4de043c5: Pull complete\n",
      "b15287f0858e: Pull complete\n",
      "507907888aac: Pull complete\n",
      "700cfc1617dd: Pull complete\n",
      "610d6205e6a8: Pull complete\n",
      "61b77f764283: Pull complete\n",
      "c891c87c58c1: Pull complete\n",
      "dfb3fd7aeafb: Pull complete\n",
      "ad892c56447b: Pull complete\n",
      "88691d388c20: Pull complete\n",
      "b007851d47d1: Pull complete\n",
      "146f07ce84d1: Pull complete\n",
      "4a90fb1ef380: Pull complete\n",
      "02324a355c24: Pull complete\n",
      "9ba4273663ee: Pull complete\n",
      "213a2b0e2402: Pull complete\n",
      "3cb09aa36b96: Pull complete\n",
      "2723fbe58239: Pull complete\n",
      "37e98b95eefd: Pull complete\n",
      "9db4a00e966a: Pull complete\n",
      "71268c98ca6d: Pull complete\n",
      "6bf954e610e1: Pull complete\n",
      "f8469a575097: Pull complete\n",
      "e9dc007ff1db: Pull complete\n",
      "4397245d6b1b: Pull complete\n",
      "Digest: sha256:c254f30c515c0168e3481e9f3ca984fc8de698630ad966a60b9462bfd538a183\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
      " ---> 8a590d7d196b\n",
      "Step 2/6 : RUN pip install -U fire cloudml-hypertune pandas==0.25.3\n",
      " ---> Running in e4cae5952dbd\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 5.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (0.1.0.dev6)\n",
      "Collecting pandas==0.25.3\n",
      "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 37.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116951 sha256=44e177389fbec671c53acae12990305898c413251a978cda2d95c315b4a22fb4\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, pandas, fire\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "Successfully installed fire-0.5.0 pandas-0.25.3 termcolor-2.3.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container e4cae5952dbd\n",
      " ---> f06fe0dbb6a5\n",
      "Step 3/6 : RUN pip install -U torchtext==0.12.0\n",
      " ---> Running in 98ac39233f8f\n",
      "Collecting torchtext==0.12.0\n",
      "  Downloading torchtext-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 29.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (1.11.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (4.64.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (2.28.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0->torchtext==0.12.0) (4.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12.0) (2022.6.15)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.12.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 98ac39233f8f\n",
      " ---> 13127bdeca7c\n",
      "Step 4/6 : WORKDIR /app\n",
      " ---> Running in a06c0af19bc6\n",
      "Removing intermediate container a06c0af19bc6\n",
      " ---> 08d236066d6b\n",
      "Step 5/6 : COPY train.py .\n",
      " ---> 6802721bbb9f\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 764be06084b6\n",
      "Removing intermediate container 764be06084b6\n",
      " ---> 4caf542b0dc6\n",
      "Successfully built 4caf542b0dc6\n",
      "Successfully tagged gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image]\n",
      "fd844732719c: Preparing\n",
      "4d5d98185197: Preparing\n",
      "9b2968493e49: Preparing\n",
      "e835c6a8acc9: Preparing\n",
      "d0f503eb5103: Preparing\n",
      "2090105d38de: Preparing\n",
      "e74f6867eca5: Preparing\n",
      "c0cceb8da3b6: Preparing\n",
      "f9dd184ec603: Preparing\n",
      "4aee62086b60: Preparing\n",
      "1bd06172a32a: Preparing\n",
      "3e584b235170: Preparing\n",
      "87c655d0c002: Preparing\n",
      "468113a3db80: Preparing\n",
      "aa1bb6b42754: Preparing\n",
      "aa7c8303dfba: Preparing\n",
      "b53563712287: Preparing\n",
      "b53563712287: Preparing\n",
      "fc1fb2554a41: Preparing\n",
      "a2fc4cabb51a: Preparing\n",
      "e8b52ca691f4: Preparing\n",
      "e4d4d528cf5d: Preparing\n",
      "25ace76efa07: Preparing\n",
      "e69159dfa907: Preparing\n",
      "ce2f668df2d8: Preparing\n",
      "1c26767a76ae: Preparing\n",
      "7e2f559b3e11: Preparing\n",
      "85af48d15b8f: Preparing\n",
      "512412b9f6a5: Preparing\n",
      "f0418bde03c7: Preparing\n",
      "97800cee6822: Preparing\n",
      "98095ac9575b: Preparing\n",
      "c4ebf9d6e076: Preparing\n",
      "440df9268ba0: Preparing\n",
      "a1f6b1a8fb1a: Preparing\n",
      "9ae92ce49008: Preparing\n",
      "6d8137497d3c: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "20634b178955: Preparing\n",
      "1a5fac543081: Preparing\n",
      "a8d0c4c62eef: Preparing\n",
      "7ed9a71261c7: Preparing\n",
      "a1eeba43cdbe: Preparing\n",
      "6127942867a5: Preparing\n",
      "e592fe6d10a9: Preparing\n",
      "f42691182163: Preparing\n",
      "68016c5bb65c: Preparing\n",
      "8034550a3bbe: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "1c26767a76ae: Waiting\n",
      "7e2f559b3e11: Waiting\n",
      "85af48d15b8f: Waiting\n",
      "512412b9f6a5: Waiting\n",
      "f0418bde03c7: Waiting\n",
      "97800cee6822: Waiting\n",
      "98095ac9575b: Waiting\n",
      "c4ebf9d6e076: Waiting\n",
      "440df9268ba0: Waiting\n",
      "a1f6b1a8fb1a: Waiting\n",
      "9ae92ce49008: Waiting\n",
      "6d8137497d3c: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "20634b178955: Waiting\n",
      "1a5fac543081: Waiting\n",
      "a8d0c4c62eef: Waiting\n",
      "7ed9a71261c7: Waiting\n",
      "a1eeba43cdbe: Waiting\n",
      "6127942867a5: Waiting\n",
      "e592fe6d10a9: Waiting\n",
      "f42691182163: Waiting\n",
      "68016c5bb65c: Waiting\n",
      "8034550a3bbe: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "2090105d38de: Waiting\n",
      "e74f6867eca5: Waiting\n",
      "c0cceb8da3b6: Waiting\n",
      "f9dd184ec603: Waiting\n",
      "4aee62086b60: Waiting\n",
      "1bd06172a32a: Waiting\n",
      "3e584b235170: Waiting\n",
      "87c655d0c002: Waiting\n",
      "468113a3db80: Waiting\n",
      "aa1bb6b42754: Waiting\n",
      "aa7c8303dfba: Waiting\n",
      "b53563712287: Waiting\n",
      "fc1fb2554a41: Waiting\n",
      "a2fc4cabb51a: Waiting\n",
      "e8b52ca691f4: Waiting\n",
      "e4d4d528cf5d: Waiting\n",
      "25ace76efa07: Waiting\n",
      "e69159dfa907: Waiting\n",
      "ce2f668df2d8: Waiting\n",
      "fd844732719c: Pushed\n",
      "4d5d98185197: Pushed\n",
      "d0f503eb5103: Pushed\n",
      "9b2968493e49: Pushed\n",
      "e74f6867eca5: Pushed\n",
      "2090105d38de: Pushed\n",
      "c0cceb8da3b6: Pushed\n",
      "f9dd184ec603: Pushed\n",
      "4aee62086b60: Pushed\n",
      "e835c6a8acc9: Pushed\n",
      "3e584b235170: Pushed\n",
      "1bd06172a32a: Pushed\n",
      "468113a3db80: Pushed\n",
      "aa7c8303dfba: Pushed\n",
      "87c655d0c002: Pushed\n",
      "b53563712287: Pushed\n",
      "fc1fb2554a41: Pushed\n",
      "a2fc4cabb51a: Pushed\n",
      "e69159dfa907: Pushed\n",
      "25ace76efa07: Pushed\n",
      "ce2f668df2d8: Pushed\n",
      "e4d4d528cf5d: Pushed\n",
      "1c26767a76ae: Pushed\n",
      "7e2f559b3e11: Pushed\n",
      "85af48d15b8f: Pushed\n",
      "512412b9f6a5: Pushed\n",
      "f0418bde03c7: Pushed\n",
      "97800cee6822: Pushed\n",
      "98095ac9575b: Pushed\n",
      "440df9268ba0: Pushed\n",
      "a1f6b1a8fb1a: Pushed\n",
      "aa1bb6b42754: Pushed\n",
      "5f70bf18a086: Layer already exists\n",
      "20634b178955: Pushed\n",
      "c4ebf9d6e076: Pushed\n",
      "a8d0c4c62eef: Layer already exists\n",
      "9ae92ce49008: Pushed\n",
      "a1eeba43cdbe: Layer already exists\n",
      "6d8137497d3c: Pushed\n",
      "e592fe6d10a9: Layer already exists\n",
      "f42691182163: Layer already exists\n",
      "68016c5bb65c: Layer already exists\n",
      "8034550a3bbe: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "7ed9a71261c7: Pushed\n",
      "1a5fac543081: Pushed\n",
      "6127942867a5: Pushed\n",
      "e8b52ca691f4: Pushed\n",
      "latest: digest: sha256:e4c713bb3f537073f67335667e6eac84f5774e06adbd00d5150caba6b53f06db size: 10571\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "931d9ff3-f4ff-4186-b4a3-28165f912df9  2023-06-12T19:01:09+00:00  35M30S    gs://qwiklabs-asl-00-69fe165840f7_cloudbuild/source/1686596468.642348-67fef37a619f455cb4f9d3b03137571c.tgz  gcr.io/qwiklabs-asl-00-69fe165840f7/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddcd4597-8588-426d-b6d3-b2836e706347",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"forestcover_tuning_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19671188-615f-47cf-b9cc-dab5ea6a2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [1171725215026118656] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai hp-tuning-jobs describe 1171725215026118656 --region=us-central1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: forestcover_tuning_20230612_193643\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "CONFIG_YAML=config.yaml\n",
    "\n",
    "cat <<EOF > $CONFIG_YAML\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: accuracy\n",
    "    goal: MAXIMIZE\n",
    "  parameters:\n",
    "  - parameterId: max_iter\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 10\n",
    "      - 20\n",
    "  - parameterId: batch_size\n",
    "    integerValueSpec:\n",
    "      minValue: 16\n",
    "      maxValue: 128\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  workerPoolSpecs:  \n",
    "  - machineSpec:\n",
    "      machineType: $MACHINE_TYPE\n",
    "    replicaCount: $REPLICA_COUNT\n",
    "    containerSpec:\n",
    "      imageUri: $IMAGE_URI\n",
    "      args:\n",
    "      - --job_dir=$JOB_DIR\n",
    "      - --training_dataset_path=$TRAINING_FILE_PATH\n",
    "      - --validation_dataset_path=$VALIDATION_FILE_PATH\n",
    "      - --hptune\n",
    "EOF\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=$CONFIG_YAML \\\n",
    "    --max-trial-count=5 \\\n",
    "    --parallel-trial-count=5\n",
    "\n",
    "echo \"JOB_NAME: $JOB_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43795075-be4d-4614-9f8a-677b4529f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(job_name):\n",
    "    jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "    match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "    tuning_job = match[0] if match else None\n",
    "    return tuning_job.trials if tuning_job else None\n",
    "\n",
    "\n",
    "def get_best_trial(trials):\n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in trials]\n",
    "    best_trial = trials[metrics.index(max(metrics))]\n",
    "    return best_trial\n",
    "\n",
    "\n",
    "def retrieve_best_trial_from_job_name(jobname):\n",
    "    trials = get_trials(jobname)\n",
    "    best_trial = get_best_trial(trials)\n",
    "    return best_trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88f8bf59-acb7-45f0-ae34-ea8c77ea0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = retrieve_best_trial_from_job_name(JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96d0437e-e792-4b25-ab5b-b21fb88c7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best_trial.parameters[0].value)\n",
    "max_iter = int(best_trial.parameters[1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4db33ff5-8f45-40f6-a434-4a1ce3bcbac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/430886062887/locations/us-central1/customJobs/5245618106075709440] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/430886062887/locations/us-central1/customJobs/5245618106075709440\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/430886062887/locations/us-central1/customJobs/5245618106075709440\n",
      "The model will be exported at: gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/jobs/JOB_VERTEX_20230612_233129\n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"JOB_VERTEX_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "WORKER_POOL_SPEC = f\"\"\"\\\n",
    "machine-type={MACHINE_TYPE},\\\n",
    "replica-count={REPLICA_COUNT},\\\n",
    "container-image-uri={IMAGE_URI}\\\n",
    "\"\"\"\n",
    "\n",
    "ARGS = f\"\"\"\\\n",
    "--job_dir={JOB_DIR},\\\n",
    "--training_dataset_path={TRAINING_FILE_PATH},\\\n",
    "--validation_dataset_path={VALIDATION_FILE_PATH},\\\n",
    "--batch_size={batch_size},\\\n",
    "--max_iter={max_iter},\\\n",
    "--nohptune\\\n",
    "\"\"\"\n",
    "\n",
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "  --args={ARGS}\n",
    "\n",
    "\n",
    "print(\"The model will be exported at:\", JOB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65f8f1f2-f5a9-49e3-bb1a-c172a590e65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/jobs/JOB_VERTEX_20230612_233129/model.pt\n",
      "gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/jobs/JOB_VERTEX_20230612_233129/preprocessing.json\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "108ac06f-4c0e-48a9-8dcb-166ff0cb48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_APP_FOLDER = \"predict_app\"\n",
    "os.makedirs(PREDICT_APP_FOLDER, exist_ok=True)\n",
    "os.environ[\"PREDICT_APP_FOLDER\"] = PREDICT_APP_FOLDER\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "548985a9-72f2-40e8-83b9-d7de037bf550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predict_app/handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PREDICT_APP_FOLDER}/handler.py\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchtext.vocab import vocab\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "CATEGORICAL_FEATURES = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "NUMERICAL_FEATURES = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "    \n",
    "class ModelHandler(BaseHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._context = None\n",
    "        self.initialized = False\n",
    "        self.explain = False\n",
    "        self.target = 0\n",
    "\n",
    "\n",
    "    def _one_hot_transformer(self, feature, dictionary):\n",
    "        dictionary = vocab(OrderedDict([(cat, 1) for cat in dictionary]))\n",
    "        \n",
    "        indices = dictionary.lookup_indices(feature)\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        one_hot = F.one_hot(indices, num_classes=len(dictionary.get_itos()))\n",
    "        return torch.tensor(one_hot).float()\n",
    "\n",
    "    def _standard_scale_transformer(self, feature, mean, std):\n",
    "        standardized = (feature - np.float64(mean)) / np.float64(std)\n",
    "        return torch.tensor(standardized)[:, None].float()    \n",
    "\n",
    "\n",
    "    def initialize(self, context):\n",
    "        self._context = context\n",
    "        self.initialized = True\n",
    "                \n",
    "        #  load the model\n",
    "        self.manifest = context.manifest\n",
    "\n",
    "        properties = context.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest['model']['serializedFile']\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.pth file\")\n",
    "\n",
    "        self.model = torch.jit.load(model_pt_path)\n",
    "        \n",
    "        # Read preprocesinng setting\n",
    "        preprocessing_json_path = os.path.join(model_dir, \"preprocessing.json\")\n",
    "        with open(preprocessing_json_path) as f:\n",
    "            self.preprocessing_json = json.load(f)\n",
    "        \n",
    "        self.initialized = True\n",
    "\n",
    "\n",
    "    def preprocess(self, _data):\n",
    "        data = _data[0]\n",
    "        \n",
    "        preprocessed_data = [self._one_hot_transformer(data[c_feature], \n",
    "                                                       self.preprocessing_json[c_feature][\"dictionary\"]) \n",
    "                             for c_feature in CATEGORICAL_FEATURES]\n",
    "        preprocessed_data.extend([self._standard_scale_transformer(data[n_feature],\n",
    "                                                                   self.preprocessing_json[n_feature][\"mean\"],\n",
    "                                                                   self.preprocessing_json[n_feature][\"std\"]) \n",
    "                                  for n_feature in NUMERICAL_FEATURES])\n",
    "        preprocessed_data = torch.cat(preprocessed_data, 1)\n",
    "        \n",
    "        return preprocessed_data\n",
    "\n",
    "    def inference(self, model_input):\n",
    "        model_output = self.model.forward(model_input)\n",
    "        return model_output\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        postprocess_output = torch.argmax(inference_output, dim=1)        \n",
    "        return [postprocess_output.cpu().numpy().tolist()]\n",
    "\n",
    "    def handle(self, data, context):\n",
    "        model_input = self.preprocess(data)\n",
    "        model_output = self.inference(model_input)\n",
    "        output = self.postprocess(model_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62e13fe6-d6e3-47ec-850d-4e85ef484825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/jobs/JOB_VERTEX_20230612_233129/model.pt...\n",
      "/ [1 files][ 17.8 KiB/ 17.8 KiB]                                                \n",
      "Operation completed over 1 objects/17.8 KiB.                                     \n",
      "Copying gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/jobs/JOB_VERTEX_20230612_233129/preprocessing.json...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $JOB_DIR/model.pt ./$PREDICT_APP_FOLDER\n",
    "!gsutil cp $JOB_DIR/preprocessing.json ./$PREDICT_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6566d6c5-a213-4a09-b0f9-e6d478f430a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "torch-model-archiver -f \\\n",
    "  --model-name model \\\n",
    "  --version 1.0 \\\n",
    "  --serialized-file ./$PREDICT_APP_FOLDER/model.pt \\\n",
    "  --handler ./$PREDICT_APP_FOLDER/handler.py \\\n",
    "  --extra-files ./$PREDICT_APP_FOLDER/preprocessing.json \\\n",
    "  --export-path=./$PREDICT_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d50b653-6b17-46a4-9c89-3548bda998a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://predict_app/handler.py [Content-Type=text/x-python]...\n",
      "Copying file://predict_app/model.pt [Content-Type=application/vnd.snesdev-page-table]...\n",
      "Copying file://predict_app/preprocessing.json [Content-Type=application/json]...\n",
      "Copying file://predict_app/model.mar [Content-Type=application/octet-stream]... \n",
      "/ [4 files][ 38.1 KiB/ 38.1 KiB]                                                \n",
      "Operation completed over 4 objects/38.1 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "DEPLOY_MODEL_GCS_URI = f\"{ARTIFACT_STORE}/pytorch-model-deploy/{TIMESTAMP}\"\n",
    "\n",
    "!gsutil cp -r $PREDICT_APP_FOLDER $DEPLOY_MODEL_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c73e5495-9a11-4444-b2d7-922b9d31ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3563  2023-06-13T15:55:53Z  gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/pytorch-model-deploy/20230613_155552/handler.py#1686671753421488  metageneration=1\n",
      "     15931  2023-06-13T15:55:53Z  gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/pytorch-model-deploy/20230613_155552/model.mar#1686671753676318  metageneration=1\n",
      "     18242  2023-06-13T15:55:53Z  gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/pytorch-model-deploy/20230613_155552/model.pt#1686671753511636  metageneration=1\n",
      "      1257  2023-06-13T15:55:53Z  gs://qwiklabs-asl-00-69fe165840f7-kfp-artifact-store/pytorch-model-deploy/20230613_155552/preprocessing.json#1686671753590373  metageneration=1\n",
      "TOTAL: 4 objects, 38993 bytes (38.08 KiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -al $DEPLOY_MODEL_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4793741-e73b-4461-9508-637c87c325a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/430886062887/locations/us-central1/models/9179772002766946304/operations/4103519438018445312\n",
      "Model created. Resource name: projects/430886062887/locations/us-central1/models/9179772002766946304@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/430886062887/locations/us-central1/models/9179772002766946304@1')\n",
      "pytorch-model-20230613_155552\n",
      "projects/430886062887/locations/us-central1/models/9179772002766946304\n"
     ]
    }
   ],
   "source": [
    "model_display_name = f\"pytorch-model-{TIMESTAMP}\"\n",
    "model_description = (\n",
    "    \"PyTorch based forest cover classifier with the pre-built PyTorch image\"\n",
    ")\n",
    "serving_container_image_uri = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\"\n",
    ")\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=serving_container_image_uri,\n",
    "    artifact_uri=DEPLOY_MODEL_GCS_URI,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed919932-52ad-4b19-b334-dc1389a49fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/430886062887/locations/us-central1/endpoints/637923452296101888/operations/5750148051775782912\n",
      "Endpoint created. Resource name: projects/430886062887/locations/us-central1/endpoints/637923452296101888\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/430886062887/locations/us-central1/endpoints/637923452296101888')\n",
      "Deploying model to Endpoint : projects/430886062887/locations/us-central1/endpoints/637923452296101888\n",
      "Deploy Endpoint model backing LRO: projects/430886062887/locations/us-central1/endpoints/637923452296101888/operations/7252098527503843328\n",
      "Endpoint model deployed. Resource name: projects/430886062887/locations/us-central1/endpoints/637923452296101888\n"
     ]
    }
   ],
   "source": [
    "machine_type = \"n1-standard-4\"\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    machine_type=machine_type, accelerator_type=None, accelerator_count=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d03e25e-6a25-41ae-8bb3-f128e0e01b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[6.0]], deployed_model_id='3432466394707394560', model_version_id='1', model_resource_name='projects/430886062887/locations/us-central1/models/9179772002766946304', explanations=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances = [\n",
    "    {\n",
    "        \"Elevation\": [2841.0],\n",
    "        \"Aspect\": [45.0],\n",
    "        \"Slope\": [0.0],\n",
    "        \"Horizontal_Distance_To_Hydrology\": [644.0],\n",
    "        \"Vertical_Distance_To_Hydrology\": [282.0],\n",
    "        \"Horizontal_Distance_To_Roadways\": [1376.0],\n",
    "        \"Hillshade_9am\": [218.0],\n",
    "        \"Hillshade_Noon\": [237.0],\n",
    "        \"Hillshade_3pm\": [156.0],\n",
    "        \"Horizontal_Distance_To_Fire_Points\": [1003.0],\n",
    "        \"Wilderness_Area\": [\"Commanche\"],\n",
    "        \"Soil_Type\": [\"C4758\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint.predict(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0bad36-e531-48aa-b43d-abc34b4d9358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee548d-5480-49f7-b7d1-192d49dae8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40646028-ddf4-4213-8ca5-667be2e5a5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7a2ea-a4f7-45f4-853b-2053d1c559f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6c317-de49-4de8-813f-a658832e9962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92541312-f0a7-41d1-a05c-6b8a5c2f0e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f1d80-bf46-421b-b00a-e3ad98791105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ace4ae-0bf4-4e32-8c32-d492412d3380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8ae2f0-dfab-4390-b2a6-5cae57f29a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f5d72_row0_col0, #T_f5d72_row1_col0, #T_f5d72_row2_col0 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_f5d72_row0_col1, #T_f5d72_row1_col1, #T_f5d72_row2_col1, #T_f5d72_row3_col0, #T_f5d72_row3_col1 {\n",
       "  background-color: black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f5d72\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f5d72_level0_col0\" class=\"col_heading level0 col0\" >df1</th>\n",
       "      <th id=\"T_f5d72_level0_col1\" class=\"col_heading level0 col1\" >df2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f5d72_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f5d72_row0_col0\" class=\"data row0 col0\" >i like to shop at store a.</td>\n",
       "      <td id=\"T_f5d72_row0_col1\" class=\"data row0 col1\" >store a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5d72_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f5d72_row1_col0\" class=\"data row1 col0\" >he likes to shop at the store b.</td>\n",
       "      <td id=\"T_f5d72_row1_col1\" class=\"data row1 col1\" >store b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5d72_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f5d72_row2_col0\" class=\"data row2 col0\" >she is happy to shop at store c.</td>\n",
       "      <td id=\"T_f5d72_row2_col1\" class=\"data row2 col1\" >store c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5d72_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f5d72_row3_col0\" class=\"data row3 col0\" >we want to shop at the store d.</td>\n",
       "      <td id=\"T_f5d72_row3_col1\" class=\"data row3 col1\" >store d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2c52d78df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = [ 'i like to shop at store a.' , 'he likes to shop at the store b.', 'she is happy to shop at store c.', 'we want to shop at the store d.']\n",
    "df2 = [ 'store a', 'store b', 'store c', 'store d' ]\n",
    "df3 = [ 'like to', 'likes to shop', 'at store' ]\n",
    "\n",
    "myDataSet = list(zip(df1,df2))\n",
    "df = pd.DataFrame(data = myDataSet, columns=['df1', 'df2'])\n",
    "\n",
    "def in_statements(val):\n",
    "    for statement in df3:\n",
    "        if statement in val:\n",
    "            color = 'yellow'\n",
    "            break\n",
    "        else:\n",
    "            color = 'black'\n",
    "    return 'background-color: %s' % color\n",
    "\n",
    "df = df.style.applymap(in_statements)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6804aed-915e-4368-a597-1ab70c457a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
